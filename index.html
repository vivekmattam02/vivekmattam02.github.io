<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Vivekananda Swamy Mattam</title>
    <meta name="author" content="Vivekananda Swamy Mattam">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Vivekananda Swamy Mattam - Masters Student in Mechatronics and Robotics at NYU Tandon. Research in autonomous navigation, visual SLAM, and robot perception.">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
<table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">

            <!-- Header Section: Photo + Bio -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p class="name" style="text-align: center;">
                            Vivekananda Swamy Mattam
                        </p>
                        <p>
                            I'm a Master's student in Mechatronics and Robotics at <a href="https://engineering.nyu.edu/">NYU Tandon School of Engineering</a>.
                            Growing up in rural India, I saw firsthand the challenges of agriculture and inefficient processes.
                            When I visited my grandfather's workplace at Mahindra and watched robots handle tasks that once required
                            intense human labor, something clicked. That's when I realized robotics wasn't just about building cool
                            machines but about creating systems that genuinely solve problems and reduce human burden.
                        </p>
                        <p>
                            NYU has been an incredible learning experience. I've had the chance to work on a Bell Labs funded project
                            building high-speed navigation systems, train quadruped robots with reinforcement learning in Isaac Lab,
                            and work on visual SLAM and perception for the VIP Self-Drive project. Before coming here, I interned at
                            <a href="#">Xmachines</a>, an agricultural robotics startup, where I worked on sensor fusion and motion planning.
                        </p>
                        <p>
                            Right now, I'm looking for <strong>internship/full-time opportunities</strong> in robotics, autonomous systems,
                            or applied AI. I want to work on projects where the technology actually matters, where robots are solving
                            real problems for real people.
                        </p>
                        <p style="text-align:center">
                            <a href="mailto:vm2677@nyu.edu">Email</a> &nbsp;/&nbsp;
                            <a href="data/Vivek_Mattam_CV.pdf">CV</a> &nbsp;/&nbsp;
                            <a href="https://www.linkedin.com/in/vivek-mattam-a8590b23a">LinkedIn</a> &nbsp;/&nbsp;
                            <a href="https://github.com/vivekmattam02">GitHub</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/portfolio.jpg">
                            <img style="width:200px;height:200px;object-fit:cover;border-radius:50%;"
                                 alt="profile photo"
                                 src="images/portfolio.jpg"
                                 class="hoverZoomLink">
                        </a>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Experience Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Experience</h2>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <!-- Course Assistant -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/nyu.jpg" width="200" alt="Course Assistant">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Course Assistant - Autonomous Mobile Robots</span>
                        <br>
                        <a href="https://engineering.nyu.edu/">NYU Tandon School of Engineering</a>
                        <br>
                        <em>2024 - Present</em>
                        <br>
                        <a href="https://vivekmattam02.github.io/amr-notes/">Course Materials</a>
                        <p></p>
                        <p>
                            Built all lecture materials for a graduate robotics course from scratch. Took the professor's
                            raw audio recordings and handwritten notes and transformed them into polished lecture slides
                            with custom diagrams and visualizations.
                        </p>
                    </td>
                </tr>

                <!-- Xmachines Intern -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/xmachines.jpg" width="200" alt="Xmachines">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Robotics Engineering Intern</span>
                        <br>
                        Xmachines - Agricultural Robotics Startup
                        <br>
                        <em>2024</em>
                        <br>
                        <p></p>
                        <p>
                            Built a Flask server for real-time sensor streaming, integrating MPU-6050 IMU data and Arducam IMX219 camera feeds into a unified monitoring interface for debugging robot state during field tests.
                        </p>
                        <p>
                            Developed an object detection tunnel for the weeder robot using computer vision to classify objects by size and route them into collection boxes, automating a previously manual sorting process.
                        </p>
                        <p>
                            Also worked on ROS-based motion planning for navigation in dynamic farm environments, with embedded systems including Jetson Orin Nano, Arduino, and ESP32.
                        </p>
                        <p><em>Tech: ROS, Python, Flask, Embedded Systems, Jetson Orin Nano</em></p>
                    </td>
                </tr>

                </tbody>
            </table>

            <!-- Research/Projects Section Header -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Projects</h2>
                        <p>
                            I'm interested in autonomous navigation, visual SLAM, robot perception, and deploying
                            learning-based systems on real robots.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Projects List -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <!-- 1) High-Speed Autonomous Navigation -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/bellabs_nav.png" width="200" alt="High-Speed Navigation">
                        </div>
                        <div style="margin-top:10px;">
                            <a href="https://youtu.be/EOe9tmgbaLE" target="_blank">
                                <img src="https://img.youtube.com/vi/EOe9tmgbaLE/0.jpg" width="200" alt="Demo Video" style="border-radius:5px;">
                            </a>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">High-Speed Autonomous Navigation in Narrow Corridors</span>
                        <br>
                        <em>NYU, Bell Labs Funded (MS Project)</em>, 2024-Present
                        <br>
                        <p></p>
                        <p>
                            Developed a ROS 2-based navigation framework for an Ackermann-steered RC car operating in corridors as narrow as 1.2 meters. Built a complete simulation environment in Gazebo Ignition with a custom URDF model addressing the four-bar linkage limitation inherent to Ackermann steering.
                        </p>
                        <p>
                            Implemented camera-LiDAR fusion with optical flow-based motion tracking and ego-motion compensation. Developed a racing line optimization algorithm that utilizes 92% of corridor width through outside-inside-outside cornering. Integrated Nav2, SLAM Toolbox, EKF localization, and MPPI control into a 60+ node architecture. Currently deploying on a real Traxxas RC car.
                        </p>
                        <p><em>Tech: ROS 2 Humble, Gazebo Ignition, Nav2, SLAM Toolbox, OpenCV, Python, C++</em></p>
                        <p><a href="https://github.com/vivekmattam02/ros2-ackermann-racing-navigation">GitHub</a> | <a href="https://youtu.be/EOe9tmgbaLE">Video</a></p>
                    </td>
                </tr>

                <!-- 2) Reinforcement Learning for Quadruped -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/quadruped_rl.gif" width="200" alt="Quadruped RL">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Reinforcement Learning for Quadruped Locomotion</span>
                        <br>
                        <em>NYU</em>, 2024
                        <br>
                        <p></p>
                        <p>
                            Trained a Unitree Go2 robot to walk using PPO in NVIDIA Isaac Lab. Implemented reward shaping for smooth actions, gait coordination (Raibert heuristic), and body stability. Added an actuator friction model with domain randomization for sim-to-real transfer. The final policy tracks velocity commands at nearly 2x the baseline targets on both flat and rough terrain.
                        </p>
                        <p><em>Tools: Isaac Lab, PyTorch, PPO, NYU HPC</em> | <a href="https://github.com/Nishant-ZFYII/rob6323_go2_project">GitHub</a></p>
                    </td>
                </tr>

                <!-- 3) Lunar Autonomy Challenge -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/lunar.png" width="200" alt="Lunar Autonomy Challenge">
                        </div>
                        <div style="margin-top:10px;">
                            <a href="https://www.youtube.com/watch?v=JbqZYXgmWIo" target="_blank">
                                <img src="https://img.youtube.com/vi/JbqZYXgmWIo/0.jpg" width="200" alt="Demo Video" style="border-radius:5px;">
                            </a>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Lunar Autonomy Challenge</span>
                        <br>
                        <em>NASA-style Lunar Autonomy Challenge</em>, 2024-Present
                        <br>
                        <p></p>
                        <p>
                            Building an autonomous navigation system for a simulated lunar rover exploring unknown terrain without GPS or prior maps. The system combines ORB-SLAM3 for visual odometry with FoundationStereo for depth estimation in challenging lunar lighting.
                        </p>
                        <p>
                            Key insight: instead of treating depth as ground truth, we propagate uncertainty through the entire pipeline. The planner actively avoids shadowed craters and texture-poor surfaces where perception is unreliable. A* planning with perception-aware costmaps and pure pursuit control complete the autonomy loop.
                        </p>
                        <p><em>Tech: Python, C++, ORB-SLAM3, FoundationStereo, OpenCV, ROS2, PyTorch</em></p>
                        <p><a href="https://github.com/vivekmattam02/LAC_26">GitHub</a> | <a href="https://www.youtube.com/watch?v=JbqZYXgmWIo">Video 1</a> | <a href="https://www.youtube.com/watch?v=Z1Es9lCJMzk">Video 2</a></p>
                    </td>
                </tr>

                <!-- 4) CityWalker-EarthRover Integration -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/earthrover.jpg" width="200" alt="CityWalker EarthRover">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">CityWalker-EarthRover Integration</span>
                        <br>
                        <em>NYU, Advisor: <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Prof. Chen Feng</a></em>, 2024-Present
                        <br>
                        <p></p>
                        <p>
                            Deploying <a href="https://ai4ce.github.io/CityWalker/">CityWalker</a> (CVPR 2025), a vision-based navigation model from our lab, onto a FrodoBots EarthRover robot. The model learned urban navigation from YouTube walking videos and was never trained on this robot, making it a zero-shot transfer problem.
                        </p>
                        <p>
                            The system takes camera feeds, predicts waypoints through the model, and converts them to motor commands. Currently working on the integration layer: coordinate transforms, trajectory tracking, and controller tuning.
                        </p>
                        <p><em>Status: In Progress</em> | <a href="https://github.com/vivekmattam02/citywalker-earthrover">GitHub</a></p>
                    </td>
                </tr>

                <!-- 5) Robot Perception: VIP Self-Drive -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/vip.jpg" width="200" alt="Robot Perception VIP Self-Drive">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Robot Perception: VIP Self-Drive</span>
                        <br>
                        <em>NYU Vertically Integrated Projects</em>, Team Voyager
                        <br>
                        <p></p>
                        <p>
                            Working on autonomous indoor navigation for TurtleBot3 using only visual SLAM, no LIDAR, just a monocular camera. I handle path planning with A* and robot localization using ORB feature matching. As part of Team Voyager, we built a maze navigation system using CosPlace descriptors for place recognition and SuperGlue for geometric verification.
                        </p>
                        <p>
                            Also implemented 2D mapping with visual odometry using ORB features, multi-object tracking with YOLOv11 and ByteTrack, and wrote RANSAC plane fitting and ICP point cloud alignment from scratch for KITTI data.
                        </p>
                        <p><em>Tech: ROS 2 Humble, PyTorch, CosPlace, SuperGlue, OpenCV, YOLOv11, ByteTrack, ORB SLAM, TurtleBot3</em></p>
                    </td>
                </tr>

                <!-- 6) HSRN Robot - Data Center -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/datacenter.jpg" width="200" alt="HSRN Robot">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HSRN Robot - Data Center Automation</span>
                        <br>
                        <em>NYU</em>, 2024-Present
                        <br>
                        <p></p>
                        <p>
                            Building a joystick-controlled robot for automating data center tasks. Focus on perception models and multi-robot coordination using sensor fusion to navigate tight spaces. Using Corelink's C++ client with ROS for inter-robot communication, with plans to transition from manual control to full autonomy.
                        </p>
                        <p><em>Tech: ROS, C++, Python, Corelink, Sensor Fusion</em></p>
                    </td>
                </tr>

                <!-- 7) S.L.A.P. Hand -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/slap.jpg" width="200" alt="S.L.A.P. Hand">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">The S.L.A.P. Hand - Gesture-Controlled Robotic Hand</span>
                        <br>
                        <em>Undergraduate Major Project</em>
                        <br>
                        <p></p>
                        <p>
                            S.L.A.P. (Simultaneous Linked Articulation Project) started as my undergraduate project and evolved over time. Began with Arduino and flex sensors for finger tracking, then moved to Propeller microcontrollers for better multi-servo control, and eventually Raspberry Pi for more computational power. The biggest shift was moving from flex sensors to vision-based control using Google's Mediapipe for hand tracking. The system now includes haptic feedback for tactile sensing.
                        </p>
                        <p><em>Tech: Raspberry Pi, Arduino, Propeller, Google Mediapipe, MPU6050, Haptic Feedback</em></p>
                    </td>
                </tr>

                <!-- 8) SCARA Manipulator -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/scara.jpg" width="200" alt="SCARA Manipulator">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SCARA Manipulator Control & Planning</span>
                        <br>
                        <em>Foundations of Robotics, NYU</em>
                        <br>
                        <p></p>
                        <p>
                            Three-phase project building control systems for a 4-DOF SCARA manipulator. Implemented inverse kinematics using Jacobian methods, added real-time obstacle avoidance with Null-Space Projection, and designed dynamic control with trapezoidal velocity profiles accounting for inertia and external forces.
                        </p>
                        <p><em>Tools: MATLAB, Simulink, VR Visualization</em><br>
                        <em>Key Concepts: Inverse Kinematics, Null-Space Control, Trajectory Planning, Inverse Dynamics</em></p>
                    </td>
                </tr>

                <!-- 9) B.A.R.K. Door -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/bark.jpg" width="200" alt="B.A.R.K. Door">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">B.A.R.K. Door - IoT Pet Access System</span>
                        <br>
                        <em>Personal Project</em>
                        <br>
                        <p></p>
                        <p>
                            B.A.R.K. (Bluetooth Actuated Remote Key) Door is a smart pet door using RFID tags to recognize authorized pets and Bluetooth for manual control. Built with a BS2 microcontroller and servo mechanisms for the locking system.
                        </p>
                        <p><em>Tech: BS2, RFID, Bluetooth, IoT, Servo Mechanisms</em></p>
                    </td>
                </tr>

                <!-- 10) E.S.V.C. -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/esvc.jpg" width="200" alt="E.S.V.C.">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">E.S.V.C. - Solar-Powered Electric Vehicle</span>
                        <br>
                        <em>Electric Solar Vehicle Championship</em>, Team Solarians 4.0
                        <br>
                        <p></p>
                        <p>
                            Designed the chassis for our entry in the Electric Solar Vehicle Championship (competing against 60+ teams). Used CATIA V5 for CAD and ANSYS R16.2 for structural analysis, optimizing an AISI 4130 steel tubular frame for lightweight design while meeting safety requirements.
                        </p>
                        <p><em>Tech: CATIA V5, ANSYS R16.2</em></p>
                    </td>
                </tr>

                </tbody>
            </table>

            <!-- Skills Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Technical Skills</h2>
                        <p>
                            <strong>Languages:</strong> Python, C++, MATLAB<br>
                            <strong>Frameworks & Tools:</strong> ROS, ROS 2, PyTorch, OpenCV, TensorFlow<br>
                            <strong>Hardware & Systems:</strong> Arduino, ESP32, Jetson Orin Nano, Embedded Systems, SLAM
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Education Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Education</h2>
                        <p>
                            <strong>M.S. Mechatronics and Robotics</strong><br>
                            New York University, Tandon School of Engineering<br>
                            <em>Expected 2026</em>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Footer -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:center;font-size:small;">
                            Template from <a href="https://jonbarron.info/">Jon Barron</a>.
                            Last updated: January 2025.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

        </td>
    </tr>
    </tbody>
</table>
</body>
</html>
