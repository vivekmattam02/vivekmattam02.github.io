<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Vivekananda Swamy Mattam</title>
    <meta name="author" content="Vivekananda Swamy Mattam">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Vivekananda Swamy Mattam - Masters Student in Mechatronics and Robotics at NYU Tandon. Research in autonomous navigation, visual SLAM, and robot perception.">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
<table style="width:100%;max-width:1100px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            
            <!-- Header Section: Photo + Bio -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p class="name" style="text-align: center;">
                            Vivekananda Swamy Mattam
                        </p>
                        <p>
                            I'm a Master's student in Mechatronics and Robotics at <a href="https://engineering.nyu.edu/">NYU Tandon School of Engineering</a>. 
                            Growing up in rural India, I saw firsthand the challenges of agriculture and inefficient processes. 
                            When I visited my grandfather's workplace at Mahindra and watched robots handle tasks that once required 
                            intense human labor, something clicked. That's when I realized robotics wasn't just about building cool 
                            machines but about creating systems that genuinely solve problems and reduce human burden.
                        </p>
                        <p>
                            NYU has been an incredible learning experience. I've had the chance to work on a Bell Labs funded project 
                            building high-speed navigation systems, train quadruped robots with reinforcement learning in Isaac Lab, 
                            and work on visual SLAM and perception for the VIP Self-Drive project. Before coming here, I interned at 
                            <a href="#">Xmachines</a>, an agricultural robotics startup, where I worked on sensor fusion and motion planning.
                        </p>
                        <p>
                            Right now, I'm looking for <strong>internship/full-time opportunities</strong> in robotics, autonomous systems, 
                            or applied AI. I want to work on projects where the technology actually matters, where robots are solving 
                            real problems for real people.
                        </p>
                        <p style="text-align:center">
                            <a href="mailto:vm2677@nyu.edu">Email</a> &nbsp;/&nbsp;
                            <a href="data/Vivek_Mattam_CV.pdf">CV</a> &nbsp;/&nbsp;
                            <a href="https://www.linkedin.com/in/vivek-mattam-a8590b23a">LinkedIn</a> &nbsp;/&nbsp;
                            <a href="https://github.com/vivekmattam02">GitHub</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="images/portfolio.jpg">
                            <img style="width:200px;height:200px;object-fit:cover;border-radius:50%;"
                                 alt="profile photo"
                                 src="images/portfolio.jpg"
                                 class="hoverZoomLink">
                        </a>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Experience Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Experience</h2>
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <!-- Course Assistant -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/nyu.jpg" width="200" alt="Course Assistant">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Course Assistant - Autonomous Mobile Robots</span>
                        <br>
                        <a href="https://engineering.nyu.edu/">NYU Tandon School of Engineering</a>
                        <br>
                        <em>2024 - Present</em>
                        <br>
                        <a href="https://vivekmattam02.github.io/amr-notes/">Course Materials</a>
                        <p></p>
                        <p>
                            Built all lecture materials for a graduate robotics course from scratch. Took the professor's
                            raw audio recordings and handwritten notes and transformed them into polished lecture slides
                            with custom diagrams and visualizations. A great exercise in breaking down complex robotics
                            concepts and presenting them in ways that actually make sense to students.
                        </p>
                    </td>
                </tr>

                <!-- Xmachines Intern -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/xmachines.jpg" width="200" alt="Xmachines">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Robotics Engineering Intern</span>
                        <br>
                        Xmachines - Agricultural Robotics Startup
                        <br>
                        <em>2024</em>
                        <br>
                        <p></p>
                        <p>
                            Built a Flask server for real-time sensor streaming—integrating MPU-6050 IMU data and Arducam IMX219 camera feeds into a unified monitoring interface. This allowed the team to debug and visualize robot state in real-time during field tests.
                        </p>
                        <p>
                            Developed an object detection tunnel for the weeder robot using computer vision to classify objects by size and route them into the correct collection boxes—essentially automating the sorting process that was previously done manually.
                        </p>
                        <p>
                            Also worked on ROS-based motion planning for navigation in dynamic farm environments, with embedded systems including Jetson Orin Nano, Arduino, and ESP32.
                        </p>
                        <p><em>Tech: ROS, C++, Python, Embedded Systems, Jetson Orin Nano, Ubuntu 22.04</em></p>
                    </td>
                </tr>

                </tbody>
            </table>

            <!-- Research/Projects Section Header -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Projects</h2>
                        <p>
                            I'm interested in autonomous navigation, visual SLAM, robot perception, and deploying
                            learning-based systems on real robots.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Projects List -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>

                <!-- 1) High-Speed Autonomous Navigation -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/bellabs_nav.png" width="200" alt="High-Speed Navigation">
                        </div>
                        <div style="margin-top:10px;">
                            <a href="https://youtu.be/EOe9tmgbaLE" target="_blank">
                                <img src="https://img.youtube.com/vi/EOe9tmgbaLE/0.jpg" width="200" alt="Demo Video" style="border-radius:5px;">
                            </a>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">High-Speed Autonomous Navigation in Narrow Corridors</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>
                        <br>
                        <em>NYU, Bell Labs Funded</em>, 2024-Present
                        <br>
                        <p></p>
                        <p>
                            Developed a ROS 2-based navigation framework for an Ackermann-steered RC car operating in corridors
                            as narrow as 1.2 meters. Built a complete simulation environment in Gazebo Ignition with a custom
                            URDF model addressing the four-bar linkage limitation inherent to Ackermann steering.
                        </p>
                        <p>
                            Implemented a camera-LiDAR fusion perception system with optical flow-based motion tracking and
                            ego-motion compensation. Developed a racing line optimization algorithm that utilizes 92% of corridor
                            width through outside-inside-outside cornering geometry. Integrated the full stack with Nav2,
                            SLAM Toolbox, EKF localization, and MPPI control, creating a 60+ node architecture validated through simulation testing. Currently deploying the system on a real Traxxas RC car as part of my MS project.
                        </p>
                        <p><em>Tech: ROS 2 Humble, Gazebo Ignition, Nav2, SLAM Toolbox, OpenCV, Python, C++</em></p>
                        <p><em>Team: 3-person team at NYU, funded by Bell Labs (MS Project)</em> | <a href="https://github.com/vivekmattam02/ros2-ackermann-racing-navigation">GitHub →</a> | <a href="https://youtu.be/EOe9tmgbaLE">Video →</a></p>
                    </td>
                </tr>

                <!-- 2) Reinforcement Learning for Quadruped -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/quadruped_rl.gif" width="200" alt="Quadruped RL">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Reinforcement Learning for Quadruped Locomotion</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>
                        <br>
                        <em>NYU</em>, 2024
                        <br>
                        <p></p>
                        <p>
                            Trained a Unitree Go2 robot to walk using PPO in NVIDIA Isaac Lab. Implemented reward shaping
                            for smooth actions, gait coordination (Raibert heuristic), and body stability. Added an actuator
                            friction model with domain randomization for sim-to-real transfer. The final policy tracks velocity
                            commands at nearly 2x the baseline targets on both flat and rough terrain.
                        </p>
                        <p><em>Tools: Isaac Lab, PyTorch, PPO, NYU HPC</em> | <a href="https://github.com/Nishant-ZFYII/rob6323_go2_project">GitHub →</a></p>
                    </td>
                </tr>

                <!-- 3) Lunar Autonomy Challenge -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/lunar.png" width="200" alt="Lunar Autonomy Challenge">
                        </div>
                        <div style="margin-top:10px;">
                            <img src="images/orbslam3andfoundationstereo.png" width="200" alt="ORB-SLAM3 and FoundationStereo" style="border-radius:5px;">
                        </div>
                        <div style="margin-top:10px;">
                            <a href="https://www.youtube.com/watch?v=JbqZYXgmWIo" target="_blank">
                                <img src="https://img.youtube.com/vi/JbqZYXgmWIo/0.jpg" width="200" alt="Demo Video 1" style="border-radius:5px;">
                            </a>
                        </div>
                        <div style="margin-top:10px;">
                            <a href="https://www.youtube.com/watch?v=Z1Es9lCJMzk" target="_blank">
                                <img src="https://img.youtube.com/vi/Z1Es9lCJMzk/0.jpg" width="200" alt="Demo Video 2" style="border-radius:5px;">
                            </a>
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Lunar Autonomy Challenge - Autonomous Navigation Stack</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>
                        <br>
                        <em>NASA-style Lunar Autonomy Challenge</em>, 2024-Present
                        <br>
                        <p></p>
                        <p>
                            Developing a complete autonomous navigation system for the NASA-style Lunar Autonomy Challenge, where a simulated rover must explore unknown lunar terrain, avoid hazards, and return to its landing site without GPS or prior maps. The system integrates visual SLAM for localization, deep learning-based stereo depth estimation, and perception-aware path planning that explicitly reasons about sensor uncertainty.
                        </p>
                        <p>
                            The perception pipeline combines ORB-SLAM3 for real-time visual odometry and mapping with FoundationStereo, a foundation model for stereo matching that provides robust depth estimation in the challenging lighting conditions of the lunar environment. Unlike traditional navigation stacks that treat depth as ground truth, this system propagates uncertainty through the entire pipeline—from stereo matching confidence to voxel grid occupancy to costmap weights—enabling the planner to actively avoid regions where perception is unreliable, such as shadowed craters and texture-poor surfaces.
                        </p>
                        <p>
                            The navigation stack implements a full autonomy loop: stereo depth estimation feeds into a 3D voxel grid, which is projected to a 2D height map and converted into a perception-aware costmap that penalizes obstacles, uncertainty, and shadows. An A* planner generates paths that balance distance-to-goal against perception risk, while a trajectory smoother and pure pursuit controller execute the motion.
                        </p>
                        <p><em>Tech: Python, C++, ORB-SLAM3, FoundationStereo, OpenCV, NumPy, ROS2, PyTorch</em></p>
                        <p><a href="https://github.com/vivekmattam02/LAC_26">GitHub →</a> | <a href="https://www.youtube.com/watch?v=JbqZYXgmWIo">Video 1 →</a> | <a href="https://www.youtube.com/watch?v=Z1Es9lCJMzk">Video 2 →</a></p>
                    </td>
                </tr>

                <!-- 4) CityWalker-EarthRover Integration -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/earthrover.jpg" width="200" alt="CityWalker EarthRover">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">CityWalker-EarthRover Integration</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>
                        <br>
                        <em>NYU</em>, 2024-Present
                        <br>
                        <p></p>
                        <p>
                            Deploying a vision-based navigation model (CityWalker, CVPR 2025) from my lab onto a FrodoBots EarthRover robot. The model learned urban navigation from YouTube walking videos and was never trained on this robot—a zero-shot transfer problem.
                        </p>
                        <p>
                            The system takes camera feeds, predicts waypoints through the model, and converts them to motor commands. Currently working on the integration layer: coordinate transforms, trajectory tracking, and controller tuning.
                        </p>
                        <p><em>Status: In Progress</em> | <a href="https://github.com/vivekmattam02/citywalker-earthrover">GitHub →</a></p>
                    </td>
                </tr>

                <!-- 4) Robot Perception: VIP Self-Drive -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/vip.jpg" width="200" alt="Robot Perception VIP Self-Drive">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">Robot Perception: VIP Self-Drive</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>, Team Voyager
                        <br>
                        <em>NYU Vertically Integrated Projects</em>
                        <br>
                        <p></p>
                        <p>
                            Working on autonomous indoor navigation for TurtleBot3 using only visual SLAM—no LIDAR, just a monocular camera. I handle path planning with A* and robot localization using ORB feature matching, building systems that navigate unknown spaces with minimal sensor data. As part of Team Voyager, we built a maze navigation system using CosPlace descriptors for place recognition and SuperGlue for geometric verification, with topological graph-based planning.
                        </p>
                        <p>
                            Beyond navigation, I've worked on 2D mapping with visual odometry using ORB features, multi-object tracking with YOLOv11 and ByteTrack, and implemented RANSAC plane fitting and ICP point cloud alignment from scratch for KITTI data. Currently prepping for the Self-Drive Exploration & Navigation Challenge.
                        </p>
                        <p><em>Tech: ROS 2 Humble, PyTorch, CosPlace, SuperGlue, OpenCV, YOLOv11, ByteTrack, Open3D, ORB SLAM, TurtleBot3, Python, C++</em></p>
                    </td>
                </tr>

                <!-- 5) HSRN Robot - Data Center -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/datacenter.jpg" width="200" alt="HSRN Robot">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">HSRN Robot - Data Center Automation</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>
                        <br>
                        <em>NYU</em>, 2024-Present
                        <br>
                        <p></p>
                        <p>
                            At NYU, I'm building a joystick-controlled robot for automating data center tasks. My focus is on developing perception models and multi-robot coordination using sensor fusion to help robots navigate tight spaces and understand their environment in real time. We're using Corelink's C++ client with ROS for inter-robot communication, with plans to transition from manual control to full autonomy.
                        </p>
                        <p><em>Tech: ROS, C++, Python, Corelink, Sensor Fusion</em></p>
                    </td>
                </tr>

                <!-- 6) S.L.A.P. Hand -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/slap.jpg" width="200" alt="S.L.A.P. Hand">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">The S.L.A.P. Hand - Gesture-Controlled Robotic Hand</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>
                        <br>
                        <em>Undergraduate Major Project</em>
                        <br>
                        <p></p>
                        <p>
                            S.L.A.P. (Simultaneous Linked Articulation Project) started as my undergraduate major project and has evolved significantly over time. I began with Arduino and flex sensors for basic finger tracking, then transitioned to Propeller microcontrollers for better multi-servo control, and eventually moved to Raspberry Pi for more computational power. The biggest shift came when I moved from flex sensors to vision-based manipulation using Google's Mediapipe for hand tracking, enabling more natural gesture control without wearable sensors. The system now includes haptic feedback for tactile sensing and I'm exploring applications for handling hazardous materials.
                        </p>
                        <p><em>Tech: Raspberry Pi, Arduino, Propeller, Google Mediapipe, MPU6050, Haptic Feedback, Servo Control</em></p>
                    </td>
                </tr>

                <!-- 7) SCARA Manipulator -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/scara.jpg" width="200" alt="SCARA Manipulator">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">SCARA Manipulator Control & Planning</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>
                        <br>
                        <em>Foundations of Robotics, NYU</em>
                        <br>
                        <p></p>
                        <p>
                            A three-phase project building progressive control systems for a 4-DOF SCARA manipulator. I implemented inverse kinematics using Jacobian methods, added real-time obstacle avoidance with Null-Space Projection, and designed dynamic control with trapezoidal velocity profiles accounting for inertia and external forces. All simulations were done in MATLAB and Simulink.
                        </p>
                        <p><em>Tools: MATLAB, Simulink, SCARA Simulation, VR Visualization</em><br>
                        <em>Key Concepts: Inverse Kinematics, Null-Space Control, Trajectory Planning, Inverse Dynamics</em></p>
                    </td>
                </tr>

                <!-- 8) B.A.R.K. Door -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/bark.jpg" width="200" alt="B.A.R.K. Door">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">B.A.R.K. Door - IoT Pet Access System</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>
                        <br>
                        <em>Personal Project</em>
                        <br>
                        <p></p>
                        <p>
                            B.A.R.K. (Bluetooth Actuated Remote Key) Door is a smart pet door using RFID tags to recognize 
                            authorized pets and Bluetooth for manual control. Built with a BS2 microcontroller and servo 
                            mechanisms for the locking system. Currently exploring Wi-Fi integration and AI-based behavioral 
                            tracking for smarter automation.
                        </p>
                        <p><em>Tech: BS2, RFID, Bluetooth, IoT, Servo Mechanisms</em></p>
                    </td>
                </tr>

                <!-- 9) E.S.V.C. -->
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <div class="one">
                            <img src="images/esvc.jpg" width="200" alt="E.S.V.C.">
                        </div>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <span class="papertitle">E.S.V.C. - Solar-Powered Electric Vehicle</span>
                        <br>
                        <strong>Vivekananda Swamy Mattam</strong>, Team Solarians 4.0
                        <br>
                        <em>Electric Solar Vehicle Championship</em>
                        <br>
                        <p></p>
                        <p>
                            Designed the chassis for our entry in the Electric Solar Vehicle Championship (competing against 
                            60+ teams). Using CATIA V5 for CAD and ANSYS R16.2 for structural analysis, optimized an AISI 4130 
                            steel tubular frame to balance lightweight design with racing durability while meeting ESVC 
                            safety requirements.
                        </p>
                        <p><em>Tech: CATIA V5, ANSYS R16.2</em></p>
                    </td>
                </tr>

                </tbody>
            </table>

            <!-- Skills Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Technical Skills</h2>
                        <p>
                            <strong>Languages:</strong> Python, C++, JavaScript, TypeScript, MATLAB<br>
                            <strong>Frameworks & Tools:</strong> ROS, ROS 2, React, OpenCV, TensorFlow<br>
                            <strong>Hardware & Systems:</strong> Arduino, ESP32, Jetson Orin Nano, Embedded Systems, SLAM
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Education Section -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Education</h2>
                        <p>
                            <strong>M.S. Mechatronics and Robotics</strong><br>
                            New York University, Tandon School of Engineering<br>
                            <em>Expected 2026</em>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <!-- Footer -->
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:center;font-size:small;">
                            Template from <a href="https://jonbarron.info/">Jon Barron</a>. 
                            Last updated: January 2025.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

        </td>
    </tr>
    </tbody>
</table>
</body>
</html>
